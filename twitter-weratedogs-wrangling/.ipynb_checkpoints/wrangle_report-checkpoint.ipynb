{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this project, my task was to gather, wrangle and analyse tweets from the WeRateDogs Twitter page. I used the data provided to generate insights and visualisations from my observations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Twitter archive data containing basic information about the tweets\n",
    "- Image predictions file includes images of dogs and algorithm predictions on what dog breed was in the tweet and whether the prognosis was a dog or not\n",
    "- A twitter file, generated using the Twitter API, containing retweets and the favourite count(likes) of each tweet received. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this project, I used the following tools:\n",
    "- Jupyter notebook for coding and documenting my wrangling, cleaning, analysis and visualisation efforts\n",
    "- Excel and notepad for visual assessment \n",
    "- Google docs for writing and formatting my report\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gathering\n",
    "I started with the Twitter archive data provided as a CSV. I read this as a CSV into the jupyter notebook making it ready to use for the project.\n",
    "\n",
    "Next, I used the requests library to programmatically import the image predictions dataset from Udacity servers and wrote the contents to a file. I read this file(image_predictions) into the jupyter notebook to make it ready for use. \n",
    "\n",
    "I set up a Twitter developer account and scrapped the Twitter API for the additional data needed. I needed retweet counts and favourite counts. I added the API scrapping code to my notebook, plugged in the secret keys and got a text file containing JSON data. I read the data line by line into a list and converted the list into a data frame containing only the necessary information. \n",
    "\n",
    "### Assessing\n",
    "Here I visually and programmatically assessed the datasets to detect issues for the cleaning phase. I was to find at least eight quality issues and two tidiness issues from the datasets.\n",
    "I visually assessed the data by loading it into excel, filtering and scrolling to identify issues.\n",
    "I used programmatic assessment by loading the dataset in a jupyter cell and using functions like info(), describe(), and value_counts to understand the structure of the datasets. I identified issues for cleaning and documented them in the jupyter notebook. \n",
    "\n",
    "### Cleaning\n",
    "At this stage, the main goal is to get the datasets ready for analysis by addressing the issues identified in the assessment stage. \n",
    "I made a copy of my datasets to ensure I kept the original data intact. \n",
    "The first cleaning task involved removing retweets and keeping only the original tweets by removing tweets with an id in the column in_reply_to_status_id. \n",
    "\n",
    "I combined the image_predictions data frame with the twitter_archive data frame to get only tweets with images. I used the merge() function to merge these two data frames creating a data frame containing original tweets having images. \n",
    "\n",
    "I cleaned up the dog stages(doggo, floofer, Puppo and pupper). Some tweets had misclassified dog stages, so I changed this to keep one dog stage. I used the loc() method to redefine the misclassified dog stages. \n",
    "\n",
    "I created a single column for dog stages instead of the four columns using the melt() function. I dropped the four columns and other columns that were not necessary for my analysis.\n",
    "\n",
    "Some columns had incorrect data types, using the astype function to convert them into the correct types for analysis.\n",
    "\n",
    "I confirmed that the date of the tweets I was going to use for analysis was before 1st August 2017.  \n",
    "\n",
    "### Storing\n",
    " I combined the Twitter archive containing image predictions with the Twitter data frame containing retweet counts and favourite counts and created a master data frame for analysis. \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
